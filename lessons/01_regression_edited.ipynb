{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0112b7-da01-4ea6-9c55-c14c1e2ee53e",
   "metadata": {},
   "source": [
    "# Python Machine Learning: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c410e1-5c6d-4a12-988d-f82237a2e4d1",
   "metadata": {},
   "source": [
    "Let's first aswer the question **What is a Model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d1e9e-de04-4b70-bbd6-f0db8a4e051f",
   "metadata": {},
   "source": [
    "### Regression Def:\n",
    "Now let's talk about machine learning with **regression**. Regression is a **supervised** problem in which we use a set of features (also called independent variables or predictors) to try to predict a continuous output, or a real valued number. This is a supervised problem because we have an existing dataset in which we know what the actual outputs are for a set of samples. By showing a model enough examples, the hope is that the model can be trained to predict the output value given just the set of features, where the prediction is as close to the real value as possible.\n",
    "\n",
    "There are many ways to perform the task of regression. In this lesson, we'll focus on linear regression, and specifically ordinary least squares (OLS), which is one of the most foundational models in statistics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be12b8-d90b-4d59-bfe6-50051ff9b54d",
   "metadata": {},
   "source": [
    "## Auto MPG Dataset\n",
    "\n",
    "We're going to use the [Auto MPG dataset](https://archive.ics.uci.edu/ml/datasets/Auto+MPG) from UCI's machine learning repository. The Auto MPG dataset contains information on city-cycle fuel consumption in miles per gallon for various types of cars. Our goal is to predict the miles per gallon of different car make and models using 7 predictors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06aeba-7beb-45d5-b2b8-5e4501c77321",
   "metadata": {},
   "source": [
    "First, let's import (or install) some packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8f9a9-b535-4020-9b48-a41bab23b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you don't have these packages installed\n",
    "!pip install numpy pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f7f9f-13a3-4653-9730-4c04c5ac58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ae75e-cae0-4a51-bbe5-24baeb00a26e",
   "metadata": {},
   "source": [
    "The `auto-mpg` dataset is stored in a `.csv` file that can be accessed from the UCI repository. We've obtained a copy and made a few modifications, which we've stored in the `data` folder. We'll use `pandas` to load in the dataset by specifying the correct path. We'll start by performing some exploratory data analysis, and then build an OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a731f81-43f5-428d-9ba4-420e781f3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the auto-mpg dataset using pandas read_csv function\n",
    "data = pd.read_csv('../data/auto-mpg.csv')\n",
    "# Check out the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51299a8e-463a-42d9-b46e-bd6e4325469b",
   "metadata": {},
   "source": [
    "Below is a \"data dictionary\", containing information about each of the variables in the dataset.\n",
    "\n",
    "| Feature     | Data Type |\n",
    "| ----------- | -------- |\n",
    "| **car name** | string (unique for each instance) | \n",
    "| **mpg**     | continuous |\n",
    "| **cylinders** | multi-valued discrete |\n",
    "| **displacement** | continuous |\n",
    "| **horsepower** |  continuous | \n",
    "| **weight** | continuous | \n",
    "| **acceleration** | continuous | \n",
    "| **model year** |  multi-valued discrete | \n",
    "| **origin** |  multi-valued discrete | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70ce76-3a42-47cf-a9ae-29cc54499e1f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Why our first step should be EDA?**\n",
    "\n",
    "Because we should start by getting familiar with our data. This is an important **first step** before jumping into any modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d79ee5-2a11-41b7-9570-caa0fe252165",
   "metadata": {},
   "source": [
    "- How many samples in the dataset do we have? \n",
    "- How big or small our dataset is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb8ae0-831b-403c-80f9-195efdc618f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748d5a5f-a76e-4a7a-9c32-f73acfb501b4",
   "metadata": {},
   "source": [
    "- Let's also look at the distribution of the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9be6e8-933f-4c30-8ec8-76fcabb66a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data['mpg'].hist(grid=False, bins=np.linspace(10, 50, 10))\n",
    "ax.set_xlabel('Miles per Gallon')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a4dac-54d2-4917-b497-3d37e74d0b80",
   "metadata": {},
   "source": [
    "- How about how the MPG correlates with the predictors? We can use the corr() function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261290f-8f60-4d34-943e-49d2d10d4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c026b-53fc-4f73-8a3f-0c2f84671ded",
   "metadata": {},
   "source": [
    ">Some variables are pretty strongly correlated with miles per gallon, so there may be some predictive signal here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220e785-8458-403a-9346-1568fa56e6da",
   "metadata": {},
   "source": [
    "---\n",
    "### Challenge 1: More EDA\n",
    "\n",
    "Create the following plots, or examine the following distributions, while exploring your data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3607f72-5a7d-4fcf-9836-8c237472b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A histogram of the displacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4135cf-18ed-4783-8514-af1e742a33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A histogram of the horsepower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5179d1d0-4dad-4f75-af46-009e27416073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A histogram of the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b84ad2-f502-4524-9bbb-0963fae607d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A histogram of the acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677339b5-cf53-4edb-99a8-25c0159060bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the unique model years, and their counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305c24c-af7c-4646-b8ee-dc3a314a36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the unique origin values, and their counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307d543-a303-4268-b51d-feb1b68787a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d596be4-3eaf-432e-aa7a-f80da1768eea",
   "metadata": {},
   "source": [
    "## Creating Train and Test Splits\n",
    "\n",
    "Next, we'll want to split our dataset into training and test data. When creating the model, we need to make sure it only sees the training data. Then, we can examine how well it **generalizes** to data it hasn't seen before. The train and test split is a foundational concept in machine learning. Be sure you're confident you understand why we do this before moving forward!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f81f7-5651-4ce8-8624-1fa59dca415f",
   "metadata": {},
   "source": [
    "- A dataset is often broken up into a feature set, or **design matrix** (typically with the variable name `X`) as well as the target or response variable `y`. Both have $D$ samples, but the design matrix will have a second dimension indicating the number of features we're using for prediction.\n",
    "\n",
    "- In this case, we'll extract the output variable `mpg` from the data frame to make the `X` and `y` variables. We use a capital `X` to denote it is a `matrix` or 2-D array, and use a lowercase `y` to denote that it is a `vector`, or 1-D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae24785-8347-426a-8bf1-5c0d5730dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the response variable and car name\n",
    "X = data.drop(columns=['car name', 'mpg'])\n",
    "# Assign response variable to its own variable\n",
    "y = data['mpg'].astype(np.float64)\n",
    "# Confidence check\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60c459-2dde-4f7b-8db2-8d8d3e2f65d1",
   "metadata": {},
   "source": [
    "- Now, we perform the train/test split. The package `scikit-learn` is the most commonly used package for machine learning in Python. It provides a function we can easily use to perform this split. Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14b06e-af8f-484c-9f00-63da9c1c7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedb18a-89d4-42fe-abf6-1f01de3d22b3",
   "metadata": {},
   "source": [
    "- We commonly do an 80/20 split, where 80% of the data is used for training, and the remaining 20% is used for testing. We can customize this using the parameters of the `train_test_split` function, which you can find in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "- We typically split the data randomly. However, sometimes we want this random split to occur in a *reproducible* fashion. This might be when we're testing our code, and want the same random split every time. Or, during a workshop, when we want all participants to get the same split, so that the results look the same for everyone. A reproducible random fit can be done by setting the `random_state`, which is an input argument to `train_test_split`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8116f87f-a8c4-42cd-bd72-19b8349bba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561732fd-63c0-4f62-98c0-26f02bef4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X train shape: {X_train.shape}; y train shape: {y_train.shape}')\n",
    "print(f'X test shape: {X_test.shape}; y test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3ab0e-e62f-4a31-8dcd-e20d689823ae",
   "metadata": {},
   "source": [
    "---\n",
    "## Building the Linear Regression Model\n",
    "\n",
    "Now that we've performed the split, let's train the model! but before that..\n",
    "\n",
    "There are numerous machine learning models that can be used to model data and generate powerful predictions. These vary widely in the types of algorithms and statistical techniques that are used when building these models. Some models are purposefully built for regression problems, while others are more suited towards classification. Many models can also be used for both sets of problems with small tweaks to their algorithms.\n",
    "\n",
    "For our dataset, we'll start with the most basic (and probably most common) regression model: **linear regression, specifically Ordinary Least Squares (OLS)**. Although it's somewhat simple in structure (compared to, for example, a neural network), linear regression is a very powerful model in its own right and can be effective when applied to many regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cea360-1fb7-4d06-92e7-7fa759e8dadb",
   "metadata": {},
   "source": [
    "### What is Ordinary Least Squares?\n",
    "\n",
    "At a high level, linear regression is nothing more than finding the best straight line, or line of best fit through a set of data points that most accurately captures the pattern that exists within those data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd9774-afd8-4bf4-8101-8a7712cc4099",
   "metadata": {},
   "source": [
    "![linear-regression](../images/linear_regression_line.png)\n",
    "> The most common picture people have of OLS is in the univariate case (2-D), which looks something like the graph above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637c88b-de84-4430-9698-57a1f56c3573",
   "metadata": {},
   "source": [
    "However, we rarely predict with only a single feature! We're mostly in the multivariate case. In this scenario, where have many \"independent variable\" axes, but still one dependent variable axis. The \"line\" in this case turns into a **hyperplane** which tries to capture as much of the information about the multi-dimensional data points as possible:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496a891-078e-4b85-8f84-caf94c1fa19d",
   "metadata": {},
   "source": [
    "![linear-regression](../images/linear_regression_hyperplane.jpeg)\n",
    ">In the above example, we have two features trying to predict a third dependent variable. This is as far as we can go with visualizing OLS, because humans have a hard time visualizations higher dimensions. But the intuition is basically the same: we're trying to pick a hyperplane that minimizes the distances to the data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f602790-feea-4924-b9bf-870b1cd63573",
   "metadata": {},
   "source": [
    "### So What is The Purpose of OLS?\n",
    "The OLS aims to minimize the sum of square difference between the observed and predicted values. When we learn an OLS model, we effectively are trying to choose the slope values (also called the weights). These are often depicted mathematically as the $\\beta$ values. There is additionally an intercept term (also called the bias term), which is really just a special case of a weight, generally denoted as $\\beta_0$. The univariate equation is probably familiar to a lot of you:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= mx + b \\\\\n",
    "  &= \\beta_0 + \\beta_1 X_1\n",
    "\\end{align}$$\n",
    "\n",
    ">You may be more familiar with the $y=mx+b$ formulation, in which $m$ is the slope, and $b$ is the intercept. This is how we specify a line. All we're doing in the second line is rewriting the notation: we're calling the intercept $\\beta_0$, and the slope $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d55300-0f99-4788-abcc-118f1f6b40e8",
   "metadata": {},
   "source": [
    "We also call the feature $X_1$. We're doing this because, when we have $P$ features (i.e., the multivariate case), this can be written as:\n",
    "\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_P X_P$$\n",
    "\n",
    ">The goal of linear regression, then, is to find a combination of these $\\beta_i$ values such that we pass through or as close to as many data points as possible. In other words, we are trying to find the values of $\\beta$ that reduce or minimize the aggregate distance between our linear model and the data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc81d9c-7d26-460c-b449-a52673ac2a77",
   "metadata": {},
   "source": [
    "### How Can We Optimize This Model?\n",
    "We can formalize this into an optimization problem and pursue a strategy that is known in machine learning as minimizing the **cost function** or **objective function** or **loss**. In the case of linear regression, the cost function we are trying to minimize is the **mean squared error (MSE)** function:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "#### where:\n",
    "* $i$ refers to the data sample,\n",
    "* $N$ is the number of samples,\n",
    "* $y_i$ is the real value of the $i$th data samples,\n",
    "* $\\hat{y}_i$ is the predicted value of the $i$th data sample, obtained from the linear model.\n",
    "\n",
    "> This is where the name OLS comes from: we're trying to find the \"least squares\" solution. It's \"ordinary\" because we're making pretty simple assumptions on the model (there are variants of OLS, in which case they are no longer \"ordinary\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c109a-d0a4-46aa-b522-41633ce36bc6",
   "metadata": {},
   "source": [
    "### Summarize:\n",
    "\n",
    "* We're trying to find the best linear model for the data;\n",
    "* Finding the best linear model means finding the right $\\beta_i$ values;\n",
    "* We go about choosing these values by minimizing the mean squared error.\n",
    "The hope is, then, that these $\\beta_i$ values are good for **generalization performance**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d313da-1afc-4477-8513-c3d9f369d31d",
   "metadata": {},
   "source": [
    "## OLS in Practice\n",
    "\n",
    "The package `scikit-learn` makes it very easy to train a linear regression model. In general, `scikit-learn` models follow the same structure:\n",
    "* 1. Create a model object.\n",
    "* 2. Fit the model to the X design matrix and Y target vector.\n",
    "* 3. Analyze the fitted parameters using .coef_ or .intercept_, or use the fitted model to make predictions on the X input data using .predict.\n",
    "\n",
    "Let's train the linear regression model using `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51161b08-68b5-4330-8014-9c9e39856451",
   "metadata": {},
   "source": [
    "* Import the model you want to train (here, `LinearRegression`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcd68a-669d-48b6-a0b3-bc816fe5a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we want from sklearn's linear model module\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac3031-e36b-4d42-847d-1d3259900350",
   "metadata": {},
   "source": [
    "* Create an object for that model with chosen settings. This is *not* training the model. For example, in linear regression, you may choose a linear regression object that does or does not fit an intercept term (see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16587d33-2c8f-494d-a77c-b5f1bc2a38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object for the model\n",
    "# We use the default settings\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908d9b1-786b-44e0-82e4-60eb80ab02d3",
   "metadata": {},
   "source": [
    "* Train the model using the `fit()` function, passing in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d943e-8647-4d6b-8cb5-f3cc8a293e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using the fit function\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fdbbb-618a-4dac-9385-cacb711199e0",
   "metadata": {},
   "source": [
    "## Evaluating a Model\n",
    "\n",
    "Now that we're done with training. We can evaluate the model on new data.\n",
    "\n",
    "**Evaluate the model on new data using the `predict()` and `score()` functions**\n",
    "\n",
    "When evaluating models, it's helpful to look at how it performs on both the training and test data, separately. This gives us a sense of the generalization gap, or how much we overfit to our data. If that gap is large, that means we need to make adjustments to the model in order to make sure it learns patterns that generalize well.\n",
    "\n",
    "For regression models, the `score()` method returns the amount of variance in the output variable that can be explained by the model predictions. This is known as $R^2$, or R-squared. It has a maximum of 1, with 1 being better predictive performance. There are many other performance metrics that can be used when predicting continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa75c8-8766-4184-a123-bfe6ad5f1f4a",
   "metadata": {},
   "source": [
    "- Let's look at the $R^2$ for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce6eb7-d0c9-4d47-bc30-0bc860904604",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training R^2: {model.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850a30b-688d-49b2-8b6c-c9d47cc60250",
   "metadata": {},
   "source": [
    "* And the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7661dd-f666-4fd8-846f-f4ca320318cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test R^2: {model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef744d75-eec7-437d-9ca6-a7c2c7cbfd4d",
   "metadata": {},
   "source": [
    ">We can see here that there is pretty good predictive performance, with a noticeable generalization gap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27b722-ad6d-42c7-baa8-16e76082470e",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE):\n",
    "\n",
    "Another common metric used in regression plots is the **RMSE**. This can be calculated by simply taking the square root of the MSE. In our case, we can intrepret this as the mean error made when predicting `mpg`, as RMSE is measured in the same units as the target variable.\n",
    "\n",
    "We can get a RMSE function from `scikit-learn`, but to run it, we'll need to get predictions for each sample using the `predict()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009f48b-121c-4ffb-b709-0d92a8a1c97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the mean squared error function\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c4fa4-796d-42ad-8003-2346834261a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36fcd1-30c7-4d43-acce-7c1e3bd6af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train RMSE: {mean_squared_error(y_train, y_train_pred, squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47c75b-bbe7-4dd8-bd2a-1b5953d1ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test RMSE: {mean_squared_error(y_test, y_test_pred, squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2eb032-eaaf-462c-a549-6ab1badd08b5",
   "metadata": {},
   "source": [
    "* We can also look at the MSE directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afe922-ee93-4488-9ec8-aa1fc7262a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train MSE: {mean_squared_error(y_train, y_train_pred)}')\n",
    "print(f'Test MSE: {mean_squared_error(y_test, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6c217-67d2-4d8c-800f-83301e4566f5",
   "metadata": {},
   "source": [
    "> It can be hard to try and assess model performance from MSE or RMSE directly, which is why people often use  ð‘…2\n",
    "  to evaluate predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cbdc4-d2c7-4342-b212-f46846bc2335",
   "metadata": {},
   "source": [
    "---\n",
    "### Challenge 2: Mean Absolute Error\n",
    "\n",
    "Another commonly used metric in regression is the **Mean Absolute Error (MAE)**. As the name suggests, this can be calculated by taking the mean of the absolute errors. Calculate the mean absolute error on the training and test data with your trained model. We've imported the MAE for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b1799-94a8-454d-932f-c600becf7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07394bae-cd77-4aa1-8aec-7a6778b567a6",
   "metadata": {},
   "source": [
    "---\n",
    "## Interpreting a Model\n",
    "\n",
    "**Examine the fitted coefficients using attributes (`coef_` and `intercept_`)**\n",
    "\n",
    "The nice thing about linear models is that they're seen as \"interpretable\". That is, we can go back  and look at the resulting $\\beta$ coefficients and exactly say what the model suggests is the relationship between the featureand output variable.\n",
    "\n",
    "We can access these coefficients by using the `coef_` attribute of the fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de18cf-8a22-4d8d-9e17-1bec33ddcc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7ec36-9444-4dde-a803-956f511e36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d33911e-ab8a-44c0-94b9-eaa678aaf804",
   "metadata": {},
   "source": [
    ">For example, the first coefficient corresponding to cylinders has value equal to  âˆ’0.227\n",
    " . This suggests that, for each increase in the cylinder, there is a decrease in the MPG by  0.227\n",
    " . So, the coefficient gives us both a sense of the direction and magnitude of the relationship between feature and MPG.\n",
    "What do the other coefficients tell you about how the features relate to the output variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a3acc-7024-4797-9621-bcf8fdab6b0a",
   "metadata": {},
   "source": [
    "---\n",
    "### Challenge 3: Feature Engineering\n",
    "\n",
    "You might notice that the `origin` variable has only three values. So, it's really a categorical variable, where each sample has one of three origins. In this scenario, we've treated it like a continuous variable. \n",
    "\n",
    "How can we properly treat this variable as categorical? This is a question of preprocessing and **feature engineering**.\n",
    "\n",
    "What we can do is replace the `origin` feature with two binary variables. The first tells us whether origin is equal to 2. The second tells us whether origin is equal to 3. If both are false, that means origin is equal to 1.\n",
    "\n",
    "By fitting a linear regression with these two binary features rather than treating `origin` as continuous, we can get a better sense for how the origin impacts the MPG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ffaf2-33ac-4317-90f1-f7df154a60a7",
   "metadata": {},
   "source": [
    "**Create two new binary features corresponding to origin, and then recreate the training and test data. Then, fit a linear model to the new data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50074ba-140c-4f60-bb3e-cf9b26849878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two new binary features corresponding to origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d38bf3-c739-4602-ba80-00b8015ee239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the response variable and car name\n",
    "\n",
    "#A ssign response variable to its own variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4224ff-1674-430d-a168-2fb475e73a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "\n",
    "# Fit model\n",
    "\n",
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d14d44-5319-4bff-9be7-644920d0a190",
   "metadata": {},
   "source": [
    "> What do you find about the performance and new coefficients?\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
